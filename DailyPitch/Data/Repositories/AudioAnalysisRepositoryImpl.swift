import Foundation
import Combine
import AVFoundation

/// AudioAnalysisRepositoryì˜ êµ¬í˜„ì²´
/// FFTAnalyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì˜¤ë””ì˜¤ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µ
class AudioAnalysisRepositoryImpl: AudioAnalysisRepository {
    
    // MARK: - Properties
    
    private let fftAnalyzer: FFTAnalyzer
    private let audioEngine: AVAudioEngine
    private let inputNode: AVAudioInputNode
    private var realtimeSubject: PassthroughSubject<FrequencyData, AudioAnalysisError>?
    private var isCurrentlyAnalyzing = false
    
    // MARK: - Initialization
    
    init(fftSize: Int = 1024) {
        self.fftAnalyzer = FFTAnalyzer(fftSize: fftSize)
        self.audioEngine = AVAudioEngine()
        self.inputNode = audioEngine.inputNode
    }
    
    deinit {
        stopRealtimeAnalysis()
    }
    
    // MARK: - Singleton Pattern
    
    static func shared() -> AudioAnalysisRepositoryImpl {
        return AudioAnalysisRepositoryImpl()
    }
    
    // MARK: - AudioAnalysisRepository Implementation
    
    func analyzeAudio(from audioSession: AudioSession) -> AnyPublisher<AudioAnalysisResult, AudioAnalysisError> {
        return Future<AudioAnalysisResult, AudioAnalysisError> { [weak self] promise in
            guard let self = self else {
                promise(.failure(.fftProcessingFailed))
                return
            }
            
            guard let audioURL = audioSession.audioFileURL else {
                promise(.failure(.fileReadError))
                return
            }
            
            let startTime = Date()
            
            self.fftAnalyzer.analyzeAudioFile(at: audioURL) { result in
                switch result {
                case .success(let frequencyDataArray):
                    let analysisResult = AudioAnalysisResult(
                        audioSession: audioSession,
                        frequencyDataSequence: frequencyDataArray,
                        status: .completed,
                        analysisStartTime: startTime,
                        analysisEndTime: Date(),
                        error: nil
                    )
                    promise(.success(analysisResult))
                    
                case .failure(let error):
                    let analysisError: AudioAnalysisError
                    if error is AudioAnalysisError {
                        analysisError = error as! AudioAnalysisError
                    } else {
                        analysisError = .fftProcessingFailed
                    }
                    
                    let failedResult = AudioAnalysisResult(
                        audioSession: audioSession,
                        frequencyDataSequence: [],
                        status: .failed,
                        analysisStartTime: startTime,
                        analysisEndTime: Date(),
                        error: analysisError
                    )
                    
                    promise(.success(failedResult))
                }
            }
        }
        .eraseToAnyPublisher()
    }
    
    func analyzeAudioData(_ audioData: [Float], sampleRate: Double) -> AnyPublisher<FrequencyData, AudioAnalysisError> {
        return Future<FrequencyData, AudioAnalysisError> { [weak self] promise in
            guard let self = self else {
                promise(.failure(.fftProcessingFailed))
                return
            }
            
            // ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¶„ì„ ìˆ˜í–‰
            DispatchQueue.global(qos: .userInitiated).async {
                let frequencyData = self.fftAnalyzer.analyze(audioData: audioData, sampleRate: sampleRate)
                
                DispatchQueue.main.async {
                    promise(.success(frequencyData))
                }
            }
        }
        .eraseToAnyPublisher()
    }
    
    func startRealtimeAnalysis(sampleRate: Double) -> AnyPublisher<FrequencyData, AudioAnalysisError> {
        // ì´ë¯¸ ë¶„ì„ ì¤‘ì¸ ê²½ìš° ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ ë°˜í™˜
        if isCurrentlyAnalyzing, let subject = realtimeSubject {
            return subject.eraseToAnyPublisher()
        }
        
        let subject = PassthroughSubject<FrequencyData, AudioAnalysisError>()
        self.realtimeSubject = subject
        
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê¶Œí•œ í™•ì¸ ë° ì˜¤ë””ì˜¤ ì—”ì§„ ì„¤ì •
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else {
                DispatchQueue.main.async {
                    subject.send(completion: .failure(.fftProcessingFailed))
                }
                return
            }
            
            do {
                // 1. ê¶Œí•œ í™•ì¸
                let permissionGranted = self.checkMicrophonePermission()
                if !permissionGranted {
                    throw AudioAnalysisError.invalidAudioData
                }
                
                // 2. ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • (ë‹¨ê³„ì ìœ¼ë¡œ)
                try self.configureAudioSessionSafely(sampleRate: sampleRate)
                
                // 3. ì˜¤ë””ì˜¤ ì—”ì§„ ì„¤ì • ë° ì‹œì‘
                DispatchQueue.main.async {
                    self.setupAudioEngine(sampleRate: sampleRate, subject: subject)
                    
                    // ì˜¤ë””ì˜¤ ì—”ì§„ ì‹œì‘
                    do {
                        if !self.audioEngine.isRunning {
                            try self.audioEngine.start()
                        }
                        self.isCurrentlyAnalyzing = true
                        print("âœ… ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘ë¨ - ìƒ˜í”Œë ˆì´íŠ¸: \(sampleRate)Hz")
                    } catch {
                        print("âŒ ì˜¤ë””ì˜¤ ì—”ì§„ ì‹œì‘ ì‹¤íŒ¨: \(error)")
                        self.isCurrentlyAnalyzing = false
                        self.deactivateAudioSession()
                        subject.send(completion: .failure(.fftProcessingFailed))
                    }
                }
                
            } catch {
                print("âŒ ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • ì‹¤íŒ¨: \(error)")
                DispatchQueue.main.async {
                    self.isCurrentlyAnalyzing = false
                    self.deactivateAudioSession()
                    subject.send(completion: .failure(.fftProcessingFailed))
                }
            }
        }
        
        return subject.eraseToAnyPublisher()
    }
    
    func stopRealtimeAnalysis() {
        isCurrentlyAnalyzing = false
        
        // ì˜¤ë””ì˜¤ ì—”ì§„ ì •ë¦¬
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        // íƒ­ ì œê±° (ì•ˆì „í•˜ê²Œ)
        if audioEngine.inputNode.numberOfInputs > 0 {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // ì˜¤ë””ì˜¤ ì„¸ì…˜ í•´ì œ
        deactivateAudioSession()
        
        // Subject ì •ë¦¬
        realtimeSubject?.send(completion: .finished)
        realtimeSubject = nil
    }
    
    var isAnalyzing: Bool {
        return isCurrentlyAnalyzing
    }
    
    // MARK: - Private Methods
    
    private func setupAudioEngine(sampleRate: Double, subject: PassthroughSubject<FrequencyData, AudioAnalysisError>) {
        _ = inputNode.outputFormat(forBus: 0)
        
        // ì›í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë³€í™˜ (ëª¨ë…¸, Float32)
        guard let recordingFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: sampleRate,
            channels: 1,
            interleaved: false
        ) else {
            subject.send(completion: .failure(.invalidAudioData))
            return
        }
        
        // ë²„í¼ í¬ê¸° ì„¤ì • (1024 ìƒ˜í”Œ)
        let bufferSize: AVAudioFrameCount = 1024
        
        // ì…ë ¥ ë…¸ë“œì— íƒ­ ì„¤ì¹˜
        inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: recordingFormat) { [weak self] buffer, time in
            guard let self = self, self.isCurrentlyAnalyzing else { return }
            
            // FFT ë¶„ì„ ìˆ˜í–‰ (ê²€ì¦ëœ ë°ì´í„°ë§Œ ì „ì†¡)
            if let frequencyData = self.fftAnalyzer.analyzeBuffer(buffer, sampleRate: sampleRate) {
                // ìœ íš¨í•œ í”¼í¬ ì£¼íŒŒìˆ˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì „ì†¡
                if let peakFreq = frequencyData.peakFrequency, 
                   peakFreq >= 20.0 && peakFreq <= 20000.0 { // ì‚¬ëŒì´ ë“¤ì„ ìˆ˜ ìˆëŠ” ì£¼íŒŒìˆ˜ ë²”ìœ„
                    DispatchQueue.main.async {
                        subject.send(frequencyData)
                    }
                } else {
                    // ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ë¡œê·¸ë§Œ ì¶œë ¥
                    let peakFreqString = frequencyData.peakFrequency.map { String(format: "%.2f", $0) } ?? "ì—†ìŒ"
                    print("âš ï¸ ì‹¤ì‹œê°„ ë¶„ì„: ìœ íš¨í•˜ì§€ ì•Šì€ ì£¼íŒŒìˆ˜ ë°ì´í„° ë¬´ì‹œ (í”¼í¬: \(peakFreqString)Hz)")
                }
            }
        }
    }
}

// MARK: - AudioAnalysisRepositoryImpl Extensions

extension AudioAnalysisRepositoryImpl {
    
    /// ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
    private func checkMicrophonePermission() -> Bool {
        // iOS 17.0 ì´ìƒì—ì„œëŠ” AVAudioApplication ì‚¬ìš©
        if #available(iOS 17.0, *) {
            return AVAudioApplication.shared.recordPermission == .granted
        } else {
            return AVAudioSession.sharedInstance().recordPermission == .granted
        }
    }
    
    /// ì•ˆì „í•œ ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì •
    /// - Parameter sampleRate: ì›í•˜ëŠ” ìƒ˜í”Œ ë ˆì´íŠ¸
    private func configureAudioSessionSafely(sampleRate: Double) throws {
        let audioSession = AVAudioSession.sharedInstance()
        
        print("ğŸ”§ ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • ì‹œì‘...")
        
        // 1. ê¸°ì¡´ ì„¸ì…˜ ì •ë¦¬
        if audioSession.isOtherAudioPlaying {
            print("ğŸ”§ ë‹¤ë¥¸ ì˜¤ë””ì˜¤ ì„¸ì…˜ ë¹„í™œì„±í™” ì¤‘...")
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
            // ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ì„¸ì…˜ ì „í™˜ ì™„ë£Œ
            Thread.sleep(forTimeInterval: 0.1)
        }
        
        // 2. ì¹´í…Œê³ ë¦¬ ì„¤ì • (ë‹¨ê³„ë³„)
        print("ğŸ”§ ì¹´í…Œê³ ë¦¬ ì„¤ì •: .playAndRecord")
        try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
        
        // 3. ìƒ˜í”Œ ë ˆì´íŠ¸ ì„¤ì •
        print("ğŸ”§ ìƒ˜í”Œ ë ˆì´íŠ¸ ì„¤ì •: \(sampleRate)Hz")
        try audioSession.setPreferredSampleRate(sampleRate)
        
        // 4. ë²„í¼ ì§€ì† ì‹œê°„ ì„¤ì •
        let bufferDuration = 1024.0 / sampleRate // ë™ì  ê³„ì‚°
        print("ğŸ”§ ë²„í¼ ì§€ì† ì‹œê°„ ì„¤ì •: \(bufferDuration)ì´ˆ")
        try audioSession.setPreferredIOBufferDuration(bufferDuration)
        
        // 5. ì„¸ì…˜ í™œì„±í™”
        print("ğŸ”§ ì˜¤ë””ì˜¤ ì„¸ì…˜ í™œì„±í™” ì¤‘...")
        try audioSession.setActive(true, options: [])
        
        print("âœ… ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")
    }
    
    /// ê¸°ì¡´ ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • (í˜¸í™˜ì„± ìœ ì§€)
    /// - Parameter sampleRate: ì›í•˜ëŠ” ìƒ˜í”Œ ë ˆì´íŠ¸
    private func configureAudioSession(sampleRate: Double) throws {
        try configureAudioSessionSafely(sampleRate: sampleRate)
    }
    
    /// ì˜¤ë””ì˜¤ ì„¸ì…˜ í•´ì œ
    private func deactivateAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("Failed to deactivate audio session: \(error)")
        }
    }
    
    /// ì‹¤ì‹œê°„ ë¶„ì„ì„ ìœ„í•œ ê³ ê¸‰ ì„¤ì •
    /// - Parameters:
    ///   - sampleRate: ìƒ˜í”Œ ë ˆì´íŠ¸
    ///   - bufferSize: ë²„í¼ í¬ê¸°
    ///   - windowOverlap: ìœˆë„ìš° ê²¹ì¹¨ ë¹„ìœ¨ (0.0 ~ 1.0)
    func startAdvancedRealtimeAnalysis(
        sampleRate: Double,
        bufferSize: AVAudioFrameCount = 1024,
        windowOverlap: Float = 0.5
    ) -> AnyPublisher<FrequencyData, AudioAnalysisError> {
        
        let subject = PassthroughSubject<FrequencyData, AudioAnalysisError>()
        self.realtimeSubject = subject
        self.isCurrentlyAnalyzing = true
        
        // ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • ì‹œë„
        do {
            try configureAudioSession(sampleRate: sampleRate)
        } catch {
            subject.send(completion: .failure(.invalidAudioData))
            return subject.eraseToAnyPublisher()
        }
        
        // ê³ ê¸‰ ì˜¤ë””ì˜¤ ì—”ì§„ ì„¤ì •
        setupAdvancedAudioEngine(
            sampleRate: sampleRate,
            bufferSize: bufferSize,
            windowOverlap: windowOverlap,
            subject: subject
        )
        
        // ì˜¤ë””ì˜¤ ì—”ì§„ ì‹œì‘
        do {
            try audioEngine.start()
        } catch {
            isCurrentlyAnalyzing = false
            deactivateAudioSession()
            subject.send(completion: .failure(.fftProcessingFailed))
        }
        
        return subject.eraseToAnyPublisher()
    }
    
    private func setupAdvancedAudioEngine(
        sampleRate: Double,
        bufferSize: AVAudioFrameCount,
        windowOverlap: Float,
        subject: PassthroughSubject<FrequencyData, AudioAnalysisError>
    ) {
        _ = inputNode.outputFormat(forBus: 0)
        
        // ì›í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë³€í™˜
        guard let recordingFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: sampleRate,
            channels: 1,
            interleaved: false
        ) else {
            subject.send(completion: .failure(.invalidAudioData))
            return
        }
        
        // ê²¹ì¹˜ëŠ” ìœˆë„ìš°ë¥¼ ìœ„í•œ ë²„í¼ ê´€ë¦¬
        var previousBuffer: [Float] = []
        let hopSize = Int(Float(bufferSize) * (1.0 - windowOverlap))
        
        inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: recordingFormat) { [weak self] buffer, time in
            guard let self = self, self.isCurrentlyAnalyzing else { return }
            
            guard let channelData = buffer.floatChannelData else { return }
            let frameCount = Int(buffer.frameLength)
            let currentBuffer = Array(UnsafeBufferPointer(start: channelData[0], count: frameCount))
            
            // ê²¹ì¹˜ëŠ” ìœˆë„ìš° ì²˜ë¦¬
            let combinedBuffer = previousBuffer + currentBuffer
            
            if combinedBuffer.count >= Int(bufferSize) {
                // ë¶„ì„ì„ ìœ„í•œ ë²„í¼ ì¤€ë¹„ (í˜„ì¬ëŠ” ì›ë³¸ buffer ì‚¬ìš©)
                
                if let frequencyData = self.fftAnalyzer.analyzeBuffer(buffer, sampleRate: sampleRate) {
                    DispatchQueue.main.async {
                        subject.send(frequencyData)
                    }
                }
                
                // ë‹¤ìŒ ìœˆë„ìš°ë¥¼ ìœ„í•´ ì¼ë¶€ ë°ì´í„° ë³´ì¡´
                if combinedBuffer.count > hopSize {
                    previousBuffer = Array(combinedBuffer.suffix(combinedBuffer.count - hopSize))
                } else {
                    previousBuffer = []
                }
            } else {
                previousBuffer = combinedBuffer
            }
        }
    }
}

// MARK: - Convenience Factory Methods

extension AudioAnalysisRepositoryImpl {
    
    /// ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ AudioAnalysisRepository ìƒì„±
    static func standard() -> AudioAnalysisRepositoryImpl {
        return AudioAnalysisRepositoryImpl(fftSize: 1024)
    }
    
    /// ê³ í•´ìƒë„ ë¶„ì„ì„ ìœ„í•œ ì„¤ì •
    static func highResolution() -> AudioAnalysisRepositoryImpl {
        return AudioAnalysisRepositoryImpl(fftSize: 2048)
    }
    
    /// ì‹¤ì‹œê°„ ìµœì í™” ì„¤ì •
    static func realtimeOptimized() -> AudioAnalysisRepositoryImpl {
        return AudioAnalysisRepositoryImpl(fftSize: 512)
    }
} 