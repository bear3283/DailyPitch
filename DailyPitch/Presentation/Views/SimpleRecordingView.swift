import SwiftUI
import Combine
import AVFoundation

/// ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ ë…¹ìŒ í™”ë©´
/// í•µì‹¬ ê¸°ëŠ¥ì—ë§Œ ì§‘ì¤‘: ë…¹ìŒ â†’ ë¶„ì„ â†’ ì¬ìƒ
struct SimpleRecordingView: View {
    
    // MARK: - Properties
    @StateObject private var viewModel = SimpleRecordingViewModel()
    @State private var showingPermissionAlert = false
    
    // MARK: - Body
    
    var body: some View {
        NavigationStack {
            VStack(spacing: 32) {
                Spacer(minLength: 20)
                
                // ì•± ë¡œê³  ë° ì œëª© (ë” ì»´íŒ©íŠ¸í•˜ê²Œ)
                headerSection
                
                // ë…¹ìŒ ìƒíƒœ í‘œì‹œ
                statusSection
                
                // ë©”ì¸ ë…¹ìŒ ë²„íŠ¼
                recordingButton
                
                // ê²°ê³¼ ì„¹ì…˜ (ë…¹ìŒ ì™„ë£Œ í›„)
                if viewModel.hasResult {
                    resultSection
                        .transition(.opacity.combined(with: .move(edge: .bottom)))
                }
                
                Spacer(minLength: 20)
            }
            .padding(.horizontal, 20)
            .background(Color(.systemBackground))
            .navigationTitle("DailyPitch")
            .navigationBarTitleDisplayMode(.large)
            .alert("ë§ˆì´í¬ ê¶Œí•œ í•„ìš”", isPresented: $showingPermissionAlert) {
                Button("ì„¤ì •ìœ¼ë¡œ ì´ë™") {
                    openAppSettings()
                }
                Button("ì·¨ì†Œ", role: .cancel) { }
            } message: {
                Text("ìŒì„±ì„ ë…¹ìŒí•˜ë ¤ë©´ ë§ˆì´í¬ ì‚¬ìš© ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
            }
        }
        .animation(.easeInOut(duration: 0.3), value: viewModel.hasResult)
        .onReceive(viewModel.$permissionDenied) { denied in
            showingPermissionAlert = denied
        }
    }
    
    // MARK: - Header Section
    
    private var headerSection: some View {
        VStack(spacing: 12) {
            // ì•± ì•„ì´ì½˜ (ë” ì‘ê²Œ)
            ZStack {
                Circle()
                    .fill(
                        LinearGradient(
                            gradient: Gradient(colors: [Color.blue, Color.purple]),
                            startPoint: .topLeading,
                            endPoint: .bottomTrailing
                        )
                    )
                    .frame(width: 70, height: 70)
                
                Image(systemName: "waveform.circle.fill")
                    .font(.system(size: 35))
                    .foregroundColor(.white)
            }
            
            VStack(spacing: 6) {
                Text("DailyPitch")
                    .font(.title)
                    .fontWeight(.bold)
                    .foregroundColor(.primary)
                
                Text("ì¼ìƒì˜ ì†Œë¦¬ë¥¼ ìŒê³„ë¡œ ë³€í™˜í•˜ì„¸ìš”")
                    .font(.subheadline)
                    .foregroundColor(.secondary)
                    .multilineTextAlignment(.center)
                    .lineLimit(2)
            }
        }
    }
    
    // MARK: - Status Section
    
    private var statusSection: some View {
        VStack(spacing: 16) {
            // ìƒíƒœ ì•„ì´ì½˜
            ZStack {
                Circle()
                    .fill(statusBackgroundColor)
                    .frame(width: 110, height: 110)
                
                Image(systemName: statusIconName)
                    .font(.system(size: 45))
                    .foregroundColor(statusIconColor)
                    .symbolEffect(.pulse, isActive: viewModel.isRecording)
            }
            
            // ìƒíƒœ í…ìŠ¤íŠ¸
            VStack(spacing: 6) {
                Text(statusTitle)
                    .font(.title3)
                    .fontWeight(.semibold)
                    .foregroundColor(.primary)
                
                Text(statusDescription)
                    .font(.subheadline)
                    .foregroundColor(.secondary)
                    .multilineTextAlignment(.center)
                    .lineLimit(2)
                
                // ë…¹ìŒ ì‹œê°„ í‘œì‹œ
                if viewModel.isRecording {
                    Text(viewModel.recordingTimeString)
                        .font(.title2)
                        .fontWeight(.medium)
                        .foregroundColor(.blue)
                        .monospacedDigit()
                        .padding(.top, 4)
                }
                
                // ë¶„ì„ ì§„í–‰ë¥ 
                if viewModel.isAnalyzing {
                    VStack(spacing: 8) {
                        ProgressView(value: viewModel.analysisProgress, total: 1.0)
                            .progressViewStyle(LinearProgressViewStyle())
                            .frame(width: 180)
                        
                        Text("\(Int(viewModel.analysisProgress * 100))%")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                    .padding(.top, 8)
                }
            }
        }
    }
    
    // MARK: - Recording Button
    
    private var recordingButton: some View {
        Button(action: viewModel.toggleRecording) {
            ZStack {
                Circle()
                    .fill(buttonBackgroundColor)
                    .frame(width: 90, height: 90)
                    .shadow(color: buttonBackgroundColor.opacity(0.3), radius: 8, x: 0, y: 4)
                
                Image(systemName: buttonIconName)
                    .font(.system(size: 32, weight: .medium))
                    .foregroundColor(.white)
            }
            .scaleEffect(viewModel.isRecording ? 1.1 : 1.0)
            .animation(.easeInOut(duration: 0.2), value: viewModel.isRecording)
        }
        .disabled(viewModel.isAnalyzing)
        .buttonStyle(PlainButtonStyle())
    }
    
    // MARK: - Result Section
    
    private var resultSection: some View {
        VStack(spacing: 16) {
            Divider()
                .padding(.horizontal, -20)
            
            VStack(spacing: 14) {
                HStack {
                    Image(systemName: "music.note")
                        .foregroundColor(.blue)
                        .font(.system(size: 18, weight: .medium))
                    Text("ë¶„ì„ ê²°ê³¼")
                        .font(.headline)
                        .fontWeight(.semibold)
                    Spacer()
                }
                
                if let detectedNote = viewModel.detectedNote {
                    VStack(spacing: 10) {
                        HStack(spacing: 16) {
                            VStack(alignment: .leading, spacing: 4) {
                                Text("ê°ì§€ëœ ìŒê³„")
                                    .font(.caption)
                                    .foregroundColor(.secondary)
                                Text(detectedNote)
                                    .font(.title2)
                                    .fontWeight(.bold)
                                    .foregroundColor(.primary)
                            }
                            
                            Spacer()
                            
                            if let frequency = viewModel.peakFrequency {
                                VStack(alignment: .trailing, spacing: 4) {
                                    Text("ì£¼íŒŒìˆ˜")
                                        .font(.caption)
                                        .foregroundColor(.secondary)
                                    Text("\(String(format: "%.1f", frequency)) Hz")
                                        .font(.title3)
                                        .fontWeight(.medium)
                                        .foregroundColor(.primary)
                                }
                            }
                        }
                        .padding(16)
                        .background(
                            RoundedRectangle(cornerRadius: 12)
                                .fill(Color(.systemGray6))
                        )
                    }
                }
                
                // ì¬ìƒ ë²„íŠ¼ë“¤ (ë” ì»´íŒ©íŠ¸í•˜ê²Œ)
                playbackButtons
            }
        }
        .padding(.horizontal, -20)
        .padding(.horizontal, 20)
    }
    
    // MARK: - Playback Buttons
    
    private var playbackButtons: some View {
        VStack(spacing: 12) {
            HStack(spacing: 12) {
                // ì›ë³¸ ì¬ìƒ
                Button(action: viewModel.playOriginal) {
                    HStack(spacing: 8) {
                        Image(systemName: "play.circle.fill")
                            .font(.system(size: 16))
                        Text("ì›ë³¸")
                            .fontWeight(.medium)
                    }
                    .foregroundColor(.white)
                    .padding(.horizontal, 20)
                    .padding(.vertical, 12)
                    .background(Color.blue)
                    .cornerRadius(20)
                }
                .disabled(viewModel.isPlaying)
                
                // ìŒê³„ ì¬ìƒ
                Button(action: viewModel.playSynthesized) {
                    HStack(spacing: 8) {
                        Image(systemName: "music.note.list")
                            .font(.system(size: 16))
                        Text("ìŒê³„")
                            .fontWeight(.medium)
                    }
                    .foregroundColor(.white)
                    .padding(.horizontal, 20)
                    .padding(.vertical, 12)
                    .background(Color.purple)
                    .cornerRadius(20)
                }
                .disabled(viewModel.isPlaying || viewModel.synthesizedAudio == nil)
            }
            
            // í•¨ê»˜ ì¬ìƒ (ì „ì²´ ë„ˆë¹„)
            Button(action: viewModel.playMixed) {
                HStack(spacing: 8) {
                    Image(systemName: "speaker.wave.2.fill")
                        .font(.system(size: 16))
                    Text("ì›ë³¸ + ìŒê³„ í•¨ê»˜ ì¬ìƒ")
                        .fontWeight(.medium)
                }
                .foregroundColor(.white)
                .padding(.horizontal, 20)
                .padding(.vertical, 12)
                .background(Color.green)
                .cornerRadius(20)
            }
            .disabled(viewModel.isPlaying || viewModel.synthesizedAudio == nil)
        }
    }
    
    // MARK: - Helper Properties
    
    private var statusIconName: String {
        if viewModel.isRecording {
            return "mic.fill"
        } else if viewModel.isAnalyzing {
            return "waveform.path"
        } else if viewModel.hasResult {
            return "music.note"
        } else {
            return "mic"
        }
    }
    
    private var statusIconColor: Color {
        if viewModel.isRecording {
            return .white
        } else if viewModel.isAnalyzing {
            return .white
        } else if viewModel.hasResult {
            return .white
        } else {
            return .blue
        }
    }
    
    private var statusBackgroundColor: Color {
        if viewModel.isRecording {
            return .red
        } else if viewModel.isAnalyzing {
            return .orange
        } else if viewModel.hasResult {
            return .green
        } else {
            return Color(.systemGray5)
        }
    }
    
    private var statusTitle: String {
        if viewModel.isRecording {
            return "ë…¹ìŒ ì¤‘..."
        } else if viewModel.isAnalyzing {
            return "ë¶„ì„ ì¤‘..."
        } else if viewModel.hasResult {
            return "ë¶„ì„ ì™„ë£Œ"
        } else {
            return "ë…¹ìŒ ì¤€ë¹„"
        }
    }
    
    private var statusDescription: String {
        if viewModel.isRecording {
            return "ì†Œë¦¬ë¥¼ ë‚´ì–´ ì£¼ì„¸ìš”"
        } else if viewModel.isAnalyzing {
            return "ìŒê³„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤"
        } else if viewModel.hasResult {
            return "ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì¬ìƒí•´ë³´ì„¸ìš”"
        } else {
            return "ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒì„ ì‹œì‘í•˜ì„¸ìš”"
        }
    }
    
    private var buttonIconName: String {
        return viewModel.isRecording ? "stop.fill" : "mic.fill"
    }
    
    private var buttonBackgroundColor: Color {
        return viewModel.isRecording ? .red : .blue
    }
    
    // MARK: - Helper Methods
    
    private func openAppSettings() {
        if let settingsURL = URL(string: UIApplication.openSettingsURLString) {
            UIApplication.shared.open(settingsURL)
        }
    }
}

// MARK: - Simple Recording ViewModel

@MainActor
class SimpleRecordingViewModel: ObservableObject {
    
    // MARK: - Published Properties
    @Published var isRecording = false
    @Published var isAnalyzing = false
    @Published var isPlaying = false
    @Published var recordingDuration: TimeInterval = 0
    @Published var analysisProgress: Double = 0.0
    @Published var permissionDenied = false
    
    // ê²°ê³¼
    @Published var detectedNote: String?
    @Published var peakFrequency: Double?
    @Published var currentAudioSession: AudioSession?
    @Published var synthesizedAudio: SynthesizedAudio?
    
    // MARK: - Dependencies
    private let audioManager = CentralAudioManager.shared
    private let permissionManager = AudioPermissionManager()
    private let recordAudioUseCase: RecordAudioUseCase
    private let analyzeFrequencyUseCase: AnalyzeFrequencyUseCase
    private let synthesizeAudioUseCase: SynthesizeAudioUseCase
    
    private var cancellables = Set<AnyCancellable>()
    private var recordingTimer: Timer?
    
    // MARK: - Computed Properties
    
    var recordingTimeString: String {
        let minutes = Int(recordingDuration) / 60
        let seconds = Int(recordingDuration) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
    
    var hasResult: Bool {
        return detectedNote != nil
    }
    
    // MARK: - Initialization
    
    init() {
        let audioRecordingRepository = AudioRecordingRepositoryImpl()
        let audioAnalysisRepository = AudioAnalysisRepositoryImpl()
        let audioSynthesisRepository: AudioSynthesisRepository = AudioSynthesizer()
        
        self.recordAudioUseCase = RecordAudioUseCase(audioRepository: audioRecordingRepository)
        self.analyzeFrequencyUseCase = AnalyzeFrequencyUseCase(audioAnalysisRepository: audioAnalysisRepository)
        self.synthesizeAudioUseCase = SynthesizeAudioUseCase(audioSynthesisRepository: audioSynthesisRepository)
    }
    
    // MARK: - Public Methods
    
    func toggleRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    func playOriginal() {
        guard let audioSession = currentAudioSession,
              let audioURL = audioSession.audioFileURL else { return }
        
        playAudioFile(url: audioURL)
    }
    
    func playSynthesized() {
        guard let synthesizedAudio = synthesizedAudio else { 
            print("âŒ í•©ì„± ì˜¤ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 
        }
        
        do {
            // AVAudioPCMBuffer ìƒì„±
            let buffer = try createAudioBuffer(from: synthesizedAudio)
            
            isPlaying = true
            try audioManager.playAudio(buffer: buffer) { [weak self] in
                DispatchQueue.main.async {
                    self?.isPlaying = false
                }
            }
            print("ğŸµ í•©ì„± ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘: \(synthesizedAudio.musicNotes.count)ê°œ ìŒê³„")
        } catch {
            print("âŒ í•©ì„± ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨: \(error)")
            isPlaying = false
        }
    }
    
    func playMixed() {
        guard let currentAudioSession = currentAudioSession,
              let synthesizedAudio = synthesizedAudio,
              let audioURL = currentAudioSession.audioFileURL else {
            print("âŒ ë¯¹ìŠ¤ ì¬ìƒì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return
        }
        
        do {
            // ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            let audioFile = try AVAudioFile(forReading: audioURL)
            let originalBuffer = try createBuffer(from: audioFile)
            
            // í•©ì„± ì˜¤ë””ì˜¤ ë²„í¼ ìƒì„±
            let synthesizedBuffer = try createAudioBuffer(from: synthesizedAudio)
            
            // ë¯¹ì‹±ëœ ë²„í¼ ìƒì„±
            let mixedBuffer = try mixAudioBuffers(
                originalBuffer: originalBuffer,
                synthesizedBuffer: synthesizedBuffer,
                originalVolume: 0.7,
                synthesizedVolume: 0.5
            )
            
            isPlaying = true
            try audioManager.playAudio(buffer: mixedBuffer) { [weak self] in
                DispatchQueue.main.async {
                    self?.isPlaying = false
                }
            }
            print("ğŸµ ë¯¹ìŠ¤ ì¬ìƒ ì‹œì‘ (ì›ë³¸ + í•©ì„±)")
        } catch {
            print("âŒ ë¯¹ìŠ¤ ì¬ìƒ ì‹¤íŒ¨: \(error)")
            isPlaying = false
        }
    }
    
    // MARK: - Private Methods
    
    private func startRecording() {
        // ê¶Œí•œ í™•ì¸
        permissionManager.requestPermission()
            .receive(on: DispatchQueue.main)
            .sink { [weak self] granted in
                if granted {
                    self?.performRecording()
                } else {
                    self?.permissionDenied = true
                }
            }
            .store(in: &cancellables)
    }
    
    private func performRecording() {
        clearResults()
        
        recordAudioUseCase.startRecording()
            .receive(on: DispatchQueue.main)
            .sink(
                receiveCompletion: { [weak self] completion in
                    if case .failure(let error) = completion {
                        print("âŒ ë…¹ìŒ ì‹¤íŒ¨: \(error)")
                    }
                    self?.isRecording = false
                    self?.stopRecordingTimer()
                },
                receiveValue: { [weak self] audioSession in
                    self?.isRecording = true
                    self?.currentAudioSession = audioSession
                    self?.startRecordingTimer()
                }
            )
            .store(in: &cancellables)
    }
    
    private func stopRecording() {
        recordAudioUseCase.stopRecording()
            .receive(on: DispatchQueue.main)
            .sink(
                receiveCompletion: { [weak self] completion in
                    self?.isRecording = false
                    self?.stopRecordingTimer()
                    
                    if case .failure(let error) = completion {
                        print("âŒ ë…¹ìŒ ì¤‘ì§€ ì‹¤íŒ¨: \(error)")
                    }
                },
                receiveValue: { [weak self] audioSession in
                    self?.currentAudioSession = audioSession
                    self?.startAnalysis(audioSession: audioSession)
                }
            )
            .store(in: &cancellables)
    }
    
    private func startAnalysis(audioSession: AudioSession) {
        isAnalyzing = true
        analysisProgress = 0.0
        
        // ì§„í–‰ë¥  ì‹œë®¬ë ˆì´ì…˜
        let progressTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] timer in
            guard let self = self, self.isAnalyzing else {
                timer.invalidate()
                return
            }
            self.analysisProgress = min(self.analysisProgress + 0.05, 0.9)
        }
        
        analyzeFrequencyUseCase.analyzeAudioSession(audioSession)
            .receive(on: DispatchQueue.main)
            .sink(
                receiveCompletion: { [weak self] completion in
                    progressTimer.invalidate()
                    self?.isAnalyzing = false
                    self?.analysisProgress = 1.0
                    
                    if case .failure(let error) = completion {
                        print("âŒ ë¶„ì„ ì‹¤íŒ¨: \(error)")
                    }
                },
                receiveValue: { [weak self] result in
                    self?.processAnalysisResult(result)
                }
            )
            .store(in: &cancellables)
    }
    
    private func processAnalysisResult(_ result: AudioAnalysisResult) {
        // ëŒ€í‘œ ìŒê³„ ì •ë³´ ì¶”ì¶œ
        if let peakFrequency = result.averagePeakFrequency,
           let musicNote = MusicNote.from(frequency: peakFrequency, duration: 1.0, amplitude: 0.7) {
            
            self.detectedNote = musicNote.name
            self.peakFrequency = peakFrequency
            
            // í•©ì„± ì˜¤ë””ì˜¤ ìƒì„±
            synthesizeAudio(note: musicNote)
        }
    }
    
    private func synthesizeAudio(note: MusicNote) {
        synthesizeAudioUseCase.synthesizeFromFrequency(frequency: note.frequency, duration: note.duration, amplitude: note.amplitude, method: .sineWave)
            .receive(on: DispatchQueue.main)
            .sink(
                receiveCompletion: { completion in
                    if case .failure(let error) = completion {
                        print("âŒ í•©ì„± ì‹¤íŒ¨: \(error)")
                    }
                },
                receiveValue: { [weak self] synthesizedAudio in
                    self?.synthesizedAudio = synthesizedAudio
                }
            )
            .store(in: &cancellables)
    }
    
    private func playAudioFile(url: URL) {
        guard FileManager.default.fileExists(atPath: url.path) else {
            print("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: \(url.path)")
            return
        }
        
        do {
            let audioFile = try AVAudioFile(forReading: url)
            isPlaying = true
            
            try audioManager.playAudio(file: audioFile) { [weak self] in
                DispatchQueue.main.async {
                    self?.isPlaying = false
                }
            }
        } catch {
            print("âŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨: \(error)")
        }
    }
    
    private func startRecordingTimer() {
        recordingDuration = 0
        recordingTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] _ in
            self?.recordingDuration += 0.1
        }
    }
    
    private func stopRecordingTimer() {
        recordingTimer?.invalidate()
        recordingTimer = nil
    }
    
    private func clearResults() {
        detectedNote = nil
        peakFrequency = nil
        synthesizedAudio = nil
        recordingDuration = 0
        analysisProgress = 0.0
    }
    
    // MARK: - Audio Buffer Helper Methods
    
    private func createAudioBuffer(from synthesizedAudio: SynthesizedAudio) throws -> AVAudioPCMBuffer {
        let sampleRate = synthesizedAudio.sampleRate
        let frameCount = AVAudioFrameCount(synthesizedAudio.audioData.count)
        
        guard let format = AVAudioFormat(
            standardFormatWithSampleRate: sampleRate,
            channels: 1
        ) else {
            throw AudioManagerError.configurationFailed
        }
        
        guard let buffer = AVAudioPCMBuffer(
            pcmFormat: format,
            frameCapacity: frameCount
        ) else {
            throw AudioManagerError.playbackFailed
        }
        
        // ì˜¤ë””ì˜¤ ë°ì´í„° ë³µì‚¬
        buffer.frameLength = frameCount
        let channelData = buffer.floatChannelData![0]
        
        for i in 0..<Int(frameCount) {
            channelData[i] = synthesizedAudio.audioData[i]
        }
        
        return buffer
    }
    
    private func createBuffer(from audioFile: AVAudioFile) throws -> AVAudioPCMBuffer {
        let frameCount = AVAudioFrameCount(audioFile.length)
        
        guard let buffer = AVAudioPCMBuffer(
            pcmFormat: audioFile.processingFormat,
            frameCapacity: frameCount
        ) else {
            throw AudioManagerError.playbackFailed
        }
        
        try audioFile.read(into: buffer)
        return buffer
    }
    
    private func mixAudioBuffers(
        originalBuffer: AVAudioPCMBuffer,
        synthesizedBuffer: AVAudioPCMBuffer,
        originalVolume: Float,
        synthesizedVolume: Float
    ) throws -> AVAudioPCMBuffer {
        let originalFrameCount = originalBuffer.frameLength
        let synthesizedFrameCount = synthesizedBuffer.frameLength
        let maxFrameCount = max(originalFrameCount, synthesizedFrameCount)
        
        guard let format = AVAudioFormat(
            standardFormatWithSampleRate: 44100,
            channels: 1
        ) else {
            throw AudioManagerError.configurationFailed
        }
        
        guard let mixedBuffer = AVAudioPCMBuffer(
            pcmFormat: format,
            frameCapacity: maxFrameCount
        ) else {
            throw AudioManagerError.playbackFailed
        }
        
        // ë¯¹ì‹± ìˆ˜í–‰
        mixedBuffer.frameLength = maxFrameCount
        let mixedData = mixedBuffer.floatChannelData![0]
        let originalData = originalBuffer.floatChannelData![0]
        let synthesizedData = synthesizedBuffer.floatChannelData![0]
        
        for i in 0..<Int(maxFrameCount) {
            var sample: Float = 0.0
            
            // ì›ë³¸ ì˜¤ë””ì˜¤ ì¶”ê°€
            if i < Int(originalFrameCount) {
                sample += originalData[i] * originalVolume
            }
            
            // í•©ì„± ì˜¤ë””ì˜¤ ì¶”ê°€
            if i < Int(synthesizedFrameCount) {
                sample += synthesizedData[i] * synthesizedVolume
            }
            
            // í´ë¦¬í•‘ ë°©ì§€
            mixedData[i] = max(-1.0, min(1.0, sample))
        }
        
        return mixedBuffer
    }
}

// Preview
#Preview {
    SimpleRecordingView()
} 